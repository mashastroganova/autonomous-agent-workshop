{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Onboarding buddy\n",
    "\n",
    "<img src=\"media/autogen-onboarding.png\" alt=\"Image description\" width=\"700\">\n",
    "\n",
    "This notebook uses the following agents:\n",
    "\n",
    "1. __User proxy__: Represents the human user who provides an initial prompt to the agent - in our case, a new hire â­\n",
    "2. __Onboarding buddy__: An agent onboarding the user, by answering questions based on the company's internal sources and guiding through mandatory assignments for better understanding â­\n",
    "3. __Memory manager__: An agent responsible for memory management of onboarding sessions, and memories/personal preferences of the user in the database. Thanks to the memory manager, the conversation can restart exactly where it was left off â­"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Install pyautogen with llm option:\n",
    "```bash\n",
    "pip install \"pyautogen[lmm]>=0.2.3\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-cosmos\n",
      "  Using cached azure_cosmos-4.7.0-py3-none-any.whl.metadata (70 kB)\n",
      "Requirement already satisfied: azure-identity in /home/akopp/miniconda3/envs/autogen/lib/python3.11/site-packages (1.17.1)\n",
      "Requirement already satisfied: temp in /home/akopp/miniconda3/envs/autogen/lib/python3.11/site-packages (2020.7.2)\n",
      "Requirement already satisfied: azure-search-documents in /home/akopp/miniconda3/envs/autogen/lib/python3.11/site-packages (11.5.1)\n",
      "Requirement already satisfied: azure-core>=1.25.1 in /home/akopp/miniconda3/envs/autogen/lib/python3.11/site-packages (from azure-cosmos) (1.30.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/akopp/miniconda3/envs/autogen/lib/python3.11/site-packages (from azure-cosmos) (4.12.2)\n",
      "Requirement already satisfied: cryptography>=2.5 in /home/akopp/miniconda3/envs/autogen/lib/python3.11/site-packages (from azure-identity) (42.0.8)\n",
      "Requirement already satisfied: msal>=1.24.0 in /home/akopp/miniconda3/envs/autogen/lib/python3.11/site-packages (from azure-identity) (1.29.0)\n",
      "Requirement already satisfied: msal-extensions>=0.3.0 in /home/akopp/miniconda3/envs/autogen/lib/python3.11/site-packages (from azure-identity) (1.2.0)\n",
      "Requirement already satisfied: azure-common>=1.1 in /home/akopp/miniconda3/envs/autogen/lib/python3.11/site-packages (from azure-search-documents) (1.1.28)\n",
      "Requirement already satisfied: isodate>=0.6.0 in /home/akopp/miniconda3/envs/autogen/lib/python3.11/site-packages (from azure-search-documents) (0.6.1)\n",
      "Requirement already satisfied: requests>=2.21.0 in /home/akopp/miniconda3/envs/autogen/lib/python3.11/site-packages (from azure-core>=1.25.1->azure-cosmos) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in /home/akopp/miniconda3/envs/autogen/lib/python3.11/site-packages (from azure-core>=1.25.1->azure-cosmos) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/akopp/miniconda3/envs/autogen/lib/python3.11/site-packages (from cryptography>=2.5->azure-identity) (1.16.0)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in /home/akopp/miniconda3/envs/autogen/lib/python3.11/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.24.0->azure-identity) (2.8.0)\n",
      "Requirement already satisfied: portalocker<3,>=1.4 in /home/akopp/miniconda3/envs/autogen/lib/python3.11/site-packages (from msal-extensions>=0.3.0->azure-identity) (2.10.1)\n",
      "Requirement already satisfied: pycparser in /home/akopp/miniconda3/envs/autogen/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/akopp/miniconda3/envs/autogen/lib/python3.11/site-packages (from requests>=2.21.0->azure-core>=1.25.1->azure-cosmos) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/akopp/miniconda3/envs/autogen/lib/python3.11/site-packages (from requests>=2.21.0->azure-core>=1.25.1->azure-cosmos) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/akopp/miniconda3/envs/autogen/lib/python3.11/site-packages (from requests>=2.21.0->azure-core>=1.25.1->azure-cosmos) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/akopp/miniconda3/envs/autogen/lib/python3.11/site-packages (from requests>=2.21.0->azure-core>=1.25.1->azure-cosmos) (2024.7.4)\n",
      "Downloading azure_cosmos-4.7.0-py3-none-any.whl (252 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m252.1/252.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: azure-cosmos\n",
      "Successfully installed azure-cosmos-4.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install azure-cosmos azure-identity temp azure-search-documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from azure.cosmos import CosmosClient\n",
    "import autogen\n",
    "import os\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"../OAI_CONFIG_LIST.json\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4o\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "settings = {\n",
    "    'host': os.getenv('ACCOUNT_HOST'),\n",
    "    'database_id': os.getenv('COSMOS_DATABASE'),\n",
    "    'container_id': os.getenv('COSMOS_CONTAINER_EMPLOYEE'),\n",
    "    'tenant_id': os.getenv('TENANT_ID'),\n",
    "    'client_id': os.getenv('CLIENT_ID'),\n",
    "    'client_secret': os.getenv('CLIENT_SECRET')\n",
    "}\n",
    "\n",
    "\n",
    "for key, value in settings.items():\n",
    "    if value is None:\n",
    "        raise ValueError(f\"Missing environment variable for {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "onboarding_buddy_prompt = \"\"\"\n",
    "As a friendly and approachable onboarding buddy at Contoso, your task is to create a warm, personalized, and engaging onboarding experience for a new hire. Start by greeting them enthusiastically, as if welcoming a friend, and introduce yourself casually.\n",
    "Guide the new hire through each onboarding topic as if you're having a chat over coffee, breaking down complex information into bite-sized, relatable pieces. When explaining topics like code of conduct, company culture, and policies, use real-life examples and anecdotes to make the information more engaging and memorable.\n",
    "Encourage questions and open dialogue, making the new hire feel comfortable to share their thoughts and concerns. Regularly check in on their understanding in a casual manner before moving to the next topic.\n",
    "Your goal is to make the new hire feel like they're talking to a knowledgeable friend who's genuinely excited to help them settle into their new role at Contoso.\n",
    "Use the following guidelines:\n",
    "- Ask for their Employee ID to retrieve their information from the database.\n",
    "- Use the \"search\" function to access relevant information from the knowledge base about Contoso-specific processes and policies.\n",
    "For topics like code of conduct, company culture, policies etc, provide detailed explanations and examples. Do not send all the official information at once, but rather provide it in manageable portions.\n",
    "- Regularly check for understanding and encourage questions.\n",
    "- Guide the new hire through each stage of the onboarding process, ensuring all necessary steps are completed.\n",
    "- Do not move on to the next topic until the new hire has confirmed understanding.\n",
    "- Log in all the progress that employee has made/all the topics that the employee has learned as well as any relevant employee information in the database by working with the memory manager.\n",
    "- At the start of new conversation, pay attention to the memory manager's response to know what the new hire has learned so far.\n",
    "- DO NOT MAKE ANYTHING UP. If you are unsure about any information, use the \"search\" function to find the correct information.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "memory_manager_prompt = \"\"\"\n",
    "When asked to write to memory:\n",
    "a. First, read the existing memory.\n",
    "b. Evaluate if the new information is not already represented in the existing memories and is important enough to log it in.\n",
    "c. If it's new and important, rewrite the existing memories to incorporate the information.\n",
    "Memory format:\n",
    "\n",
    "Memories are formatted as a series of statements separated by the '|' character.\n",
    "Example: \"User is French| User likes dogs| User has 2 kids\"\n",
    "Updating memory:\n",
    "\n",
    "Incorporate new information by modifying existing statements or adding new ones.\n",
    "Remove outdated or contradictory information when necessary.\n",
    "Keep memories short, succinct, and accurate.\n",
    "\n",
    "DO NOT engage in any form of conversation beyond this task. DO NOT TALK TO THE USER.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Agents\n",
    "\n",
    "The user proxy agent is the new hire who would interact within the conversation. We are also interested in creating an agent who is capable of executing code. For this, we create an \"executor\" agent who will use a Docker container for code execution. Additionally, we create the main Onboarding Buddy agent and Memory Manager agent. Memory manager is solely responsible for interacting with memories, but will not participate in the conversations.\n",
    "\n",
    "We will use the \"Group Chat\" structure from autogen such that all the agents are part of the same conversation. The Chat Manager is responsible for routing the message to the correct agent. For the manager to have a better understanding of the role of every agent, we include the introductions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from autogen.coding import DockerCommandLineCodeExecutor\n",
    "\n",
    "llm_config = {\"config_list\": config_list, \"cache_seed\": 42}\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"User_proxy\",\n",
    "    system_message=\"The new hire.\",\n",
    "    human_input_mode=\"ALWAYS\",\n",
    "    code_execution_config=False\n",
    ")\n",
    "\n",
    "temp_dir = tempfile.TemporaryDirectory()\n",
    "\n",
    "executor = DockerCommandLineCodeExecutor(\n",
    "    image=\"python:3.12-slim\",\n",
    "    timeout=10,\n",
    "    work_dir=temp_dir.name\n",
    ")\n",
    "\n",
    "onboarding_buddy = autogen.ConversableAgent(\n",
    "    name=\"onboarding_buddy\",\n",
    "    system_message= onboarding_buddy_prompt,\n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "memory_manager = autogen.ConversableAgent(\n",
    "    name=\"memory_manager\",\n",
    "    system_message=memory_manager_prompt,\n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "code_executor_agent_using_docker = autogen.AssistantAgent(\n",
    "    \"code_executor_agent_docker\",\n",
    "    llm_config=False,\n",
    "    code_execution_config={\"executor\": executor},\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "groupchat = autogen.GroupChat(agents=[user_proxy, onboarding_buddy, code_executor_agent_using_docker, memory_manager], messages=[], max_round=50, send_introductions=True)\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions \n",
    "\n",
    "- Retrieve employee information from the database\n",
    "- Read, write and update memory \n",
    "- Do RAG over company-specific documents for onboarding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import azure.cosmos.exceptions as exceptions\n",
    "from azure.identity import ClientSecretCredential, DefaultAzureCredential\n",
    "\n",
    "HOST = settings['host']\n",
    "DATABASE_ID = settings['database_id']\n",
    "CONTAINER_ID = settings['container_id']\n",
    "TENANT_ID = settings['tenant_id']\n",
    "CLIENT_ID = settings['client_id']\n",
    "CLIENT_SECRET = settings['client_secret']\n",
    "\n",
    "# aad_credentials = ClientSecretCredential(\n",
    "#     tenant_id=TENANT_ID,\n",
    "#     client_id=CLIENT_ID,\n",
    "#     client_secret=CLIENT_SECRET)\n",
    "\n",
    "aad_credentials = DefaultAzureCredential()\n",
    "\n",
    "\n",
    "client = CosmosClient(HOST, aad_credentials)\n",
    "db = client.get_database_client(DATABASE_ID)\n",
    "container = db.get_container_client(CONTAINER_ID)\n",
    "\n",
    "@code_executor_agent_using_docker.register_for_execution()\n",
    "@onboarding_buddy.register_for_llm(name=\"get_employee_info\", description=\"Retrieve employee data from the database, including the start date and mandatory exam asignments.\")\n",
    "def get_employee_info(EmployeeId: str) -> str:\n",
    "    try:\n",
    "        query = f\"SELECT * FROM c WHERE c.EmployeeId = '{EmployeeId}'\"\n",
    "        items = list(container.query_items(query=query, enable_cross_partition_query=True))\n",
    "\n",
    "        if items:\n",
    "            employee_info = items[0]\n",
    "            return json.dumps(employee_info, default=str)\n",
    "        else:\n",
    "            error_message = {'error': f\"Employee with ID '{EmployeeId}' not found.\"}\n",
    "            return json.dumps(error_message)\n",
    "\n",
    "    except exceptions.CosmosHttpResponseError as e:\n",
    "        error_message = {'error': str(e)}\n",
    "        return json.dumps(error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "@code_executor_agent_using_docker.register_for_execution()\n",
    "@memory_manager.register_for_llm(name=\"read_memory\", description=\"Read the conversation history of an employee.\")\n",
    "def read_memory(employee_id: str) -> Dict[str, Any]:\n",
    "    try:\n",
    "        query = \"SELECT c.ConversationHistory FROM c WHERE c.EmployeeId = @employee_id\"\n",
    "        parameters = [{\"name\": \"@employee_id\", \"value\": employee_id}]\n",
    "        items = list(container.query_items(\n",
    "            query=query,\n",
    "            parameters=parameters,\n",
    "            enable_cross_partition_query=True\n",
    "        ))\n",
    "\n",
    "        if items:\n",
    "            conversation_history = items[0].get(\"ConversationHistory\", \"\")\n",
    "            return {\"conversation_history\": conversation_history}\n",
    "        else:\n",
    "            return {\"error\": f\"Employee with ID '{employee_id}' not found.\"}\n",
    "\n",
    "    except exceptions.CosmosHttpResponseError as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "@code_executor_agent_using_docker.register_for_execution()\n",
    "@memory_manager.register_for_llm(name=\"write_memory\", description=\"Write an additional message to the conversation history of an employee.\")\n",
    "def write_memory(employee_id: str, new_memory: str) -> Dict[str, Any]:\n",
    "    try:\n",
    "        query = \"SELECT * FROM c WHERE c.EmployeeId = @employee_id\"\n",
    "        parameters = [{\"name\": \"@employee_id\", \"value\": employee_id}]\n",
    "        items = list(container.query_items(\n",
    "            query=query,\n",
    "            parameters=parameters,\n",
    "            enable_cross_partition_query=True\n",
    "        ))\n",
    "\n",
    "        if items:\n",
    "            item = items[0]\n",
    "            current_history = item.get(\"ConversationHistory\", \"\")\n",
    "            updated_history = f\"{current_history}|{new_memory}\" if current_history else new_memory\n",
    "            item[\"ConversationHistory\"] = updated_history\n",
    "            updated_item = container.upsert_item(body=item)\n",
    "            return {\"updated_item\": updated_item}\n",
    "        else:\n",
    "            return {\"error\": f\"Employee with ID '{employee_id}' not found.\"}\n",
    "\n",
    "    except exceptions.CosmosHttpResponseError as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "@code_executor_agent_using_docker.register_for_execution()\n",
    "@memory_manager.register_for_llm(name=\"update_memory\", description=\"Update the conversation history of an employee with the new history/override.\")\n",
    "def update_memory(employee_id: str, new_history: str) -> Dict[str, Any]:\n",
    "    try:\n",
    "        query = \"SELECT * FROM c WHERE c.EmployeeId = @employee_id\"\n",
    "        parameters = [{\"name\": \"@employee_id\", \"value\": employee_id}]\n",
    "        items = list(container.query_items(\n",
    "            query=query,\n",
    "            parameters=parameters,\n",
    "            enable_cross_partition_query=True\n",
    "        ))\n",
    "\n",
    "        if items:\n",
    "            item = items[0]\n",
    "            item[\"ConversationHistory\"] = new_history\n",
    "            updated_item = container.upsert_item(body=item)\n",
    "            return {\"updated_item\": updated_item}\n",
    "        else:\n",
    "            return {\"error\": f\"Employee with ID '{employee_id}' not found.\"}\n",
    "\n",
    "    except exceptions.CosmosHttpResponseError as e:\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from typing import List, Dict\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import (\n",
    "    VectorizedQuery,\n",
    "    QueryType,\n",
    "    QueryCaptionType,\n",
    "    QueryAnswerType,\n",
    ")\n",
    "import os\n",
    "\n",
    "def get_embedding(query: str) -> List[float]:\n",
    "    client = AzureOpenAI(\n",
    "        api_key=os.getenv(\"AZURE_OAI_KEY\"),\n",
    "        azure_endpoint=os.getenv(\"AZURE_OAI_ENDPOINT\"),\n",
    "        api_version=\"2024-02-01\"\n",
    "    )\n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            input=query,\n",
    "            model=\"embedding\",\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting embedding: {e}\")\n",
    "        return []\n",
    "\n",
    "def retrieve_documentation(query: str) -> List[Dict]:\n",
    "    try:\n",
    "        embedding = get_embedding(query)\n",
    "        if not embedding:\n",
    "            return []\n",
    "\n",
    "        search_client = SearchClient(\n",
    "            endpoint=os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"],\n",
    "            index_name=\"onboarding\",\n",
    "            credential=AzureKeyCredential(os.getenv(\"AZURE_SEARCH_KEY\"))\n",
    "        )\n",
    "\n",
    "        vector_query = VectorizedQuery(\n",
    "            vector=embedding, k_nearest_neighbors=3, fields=\"contentVector\"\n",
    "        )\n",
    "\n",
    "        results = search_client.search(\n",
    "            search_text=query,\n",
    "            vector_queries=[vector_query],\n",
    "            query_type=QueryType.SEMANTIC,\n",
    "            semantic_configuration_name=\"azureml-default\",\n",
    "            query_caption=QueryCaptionType.EXTRACTIVE,\n",
    "            query_answer=QueryAnswerType.EXTRACTIVE,\n",
    "            top=3,\n",
    "        )\n",
    "\n",
    "        docs = [\n",
    "            {\n",
    "                \"id\": doc[\"id\"],\n",
    "                \"title\": doc[\"title\"],\n",
    "                \"content\": doc[\"content\"],\n",
    "                \"url\": doc[\"url\"],\n",
    "            }\n",
    "            for doc in results\n",
    "        ]\n",
    "\n",
    "        return docs\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving documentation: {e}\")\n",
    "        return []\n",
    "\n",
    "@code_executor_agent_using_docker.register_for_execution()\n",
    "@onboarding_buddy.register_for_llm(name=\"search\", description=\"Search the knowledge base for documentation to answer the question of the user.\")\n",
    "def search(query: str) -> List[Dict]:\n",
    "    return retrieve_documentation(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "Hi, I am a new hire\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: onboarding_buddy\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33monboarding_buddy\u001b[0m (to chat_manager):\n",
      "\n",
      "Hey there! Welcome to Contoso! I'm your onboarding buddy, and I'll be helping you get settled into your new role. I'm excited to get to know you and guide you through everything you need to know. ðŸ‘‹ðŸ˜Š\n",
      "\n",
      "First things first, could you please share your Employee ID with me? I'll need it to retrieve your information from our database and make sure we cover everything specific to your role. \n",
      "\n",
      "Feel free to ask any questions or share any thoughts you might have along the way!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: User_proxy\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chat_result = user_proxy.initiate_chat(manager,message=\"Hi, I am a new hire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogenstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
